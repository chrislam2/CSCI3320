{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI3320 Assignment 2 Code ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [4, 5, 12, 29, 30, 36, 36, 54, 58, 70, 72, 76, 78, 82, 87, 90, 90, 92, 95, 98]\n",
    "b = [49, 4, 28, 18, 65, 32, 1, 29, 76, 12, 26, 55, 4, 15, 95, 70, 55, 84, 14, 21]\n",
    "label = [0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "X = np.vstack((np.array(a), np.array(b)))\n",
    "Y = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = [(a[i], b[i]) for i in range(len(label)) if label[i] == 1]\n",
    "negative_samples = [(a[i], b[i]) for i in range(len(label)) if label[i] == 0]\n",
    "plt.scatter(*zip(*positive_samples), color='red', label='Positive Samples')\n",
    "plt.scatter(*zip(*negative_samples), color='blue', label='Negative Samples')\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "plt.legend()\n",
    "plt.title('Scatter Plot of Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression \n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def predict(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    predicition = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Calculate predictions\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    predicition[A > 0.5] = 1\n",
    "    \n",
    "    return predicition\n",
    "\n",
    "def calculate_accuracy(w, b, X, Y):\n",
    "    predictions = predict(w, b, X)\n",
    "    accuracy = 1 - np.mean(np.abs(predictions - Y))\n",
    "    return accuracy\n",
    "\n",
    "def calculate_error(w, b, X, Y):\n",
    "    predictions = predict(w, b, X)\n",
    "    error = np.mean(np.abs(predictions - Y))\n",
    "    return error\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "    return w, b\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Forward propagation\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    loss = -1/m * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A))\n",
    "    \n",
    "    # Backward propagation\n",
    "    dw = 1/m * np.dot(X, (A-Y).T)\n",
    "    db = 1/m * np.sum(A-Y)\n",
    "    \n",
    "    loss_grads = {\"dw\": dw, \"db\": db}\n",
    "    \n",
    "    return loss, loss_grads\n",
    "\n",
    "def optimizer(loss_grad, w, b, learning_rate):\n",
    "    dw = loss_grad[\"dw\"]\n",
    "    db = loss_grad[\"db\"]\n",
    "    \n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "\n",
    "    return w, b\n",
    "\n",
    "def train(w, b, X, Y, num_iterations, learning_rate):\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    error_history = []\n",
    "    for i in range(num_iterations):\n",
    "        loss, loss_grads = propagate(w, b, X, Y)\n",
    "        w, b = optimizer(loss_grads, w, b, learning_rate)\n",
    "        loss_history.append(loss)\n",
    "        accuracy = calculate_accuracy(w, b, X, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        error = calculate_error(w, b, X, Y)\n",
    "        error_history.append(error)\n",
    "        print(f'Epoch [{i+1}/{num_iterations}], Train Accuracy: {accuracy:.16f}, Train Error: {error:.16f}, Train Loss: {loss:.16f}')\n",
    "        \n",
    "    params = {\"w\": w, \"b\": b}\n",
    "    \n",
    "    return params, loss_history, accuracy_history, error_history\n",
    "\n",
    "def plot_accuracy_history(accuracy_history, savefig=False, savefig_str=\" \"):\n",
    "    plt.plot(accuracy_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy History')\n",
    "    if savefig:\n",
    "        plt.savefig(f'accuracy_history_{savefig_str}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_error_history(error_history, savefig=False, savefig_str=\" \"):\n",
    "    plt.plot(error_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Error History')\n",
    "    if savefig:\n",
    "        plt.savefig(f'error_history_{savefig_str}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_history(loss_history, savefig=False, savefig_str=\" \"):\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss History')\n",
    "    if savefig:\n",
    "        plt.savefig(f'loss_history_{savefig_str}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_accuracy_history_all(accuracy_history_list, label_str_list, savefig=False, savefig_str=\" \"):\n",
    "    for i, accuracy_history in enumerate(accuracy_history_list):\n",
    "        plt.plot(accuracy_history, label=label_str_list[i])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy History')\n",
    "    if (len(accuracy_history_list)) > 1:\n",
    "        plt.legend()\n",
    "    if savefig:\n",
    "        plt.savefig(f'accuracy_history_all_{savefig_str}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_error_history_all(error_history_list, label_str_list, savefig=False, savefig_str=\" \"):\n",
    "    for i, error_history in enumerate(error_history_list):\n",
    "        plt.plot(error_history, label=label_str_list[i])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Error History')\n",
    "    if (len(error_history_list)) > 1:\n",
    "        plt.legend()\n",
    "    if savefig:\n",
    "        plt.savefig(f'error_history_all_{savefig_str}.png')\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_history_all(loss_history_list, label_str_list, savefig=False, savefig_str=\" \"):\n",
    "    for i, loss_history in enumerate(loss_history_list):\n",
    "        plt.plot(loss_history, label=label_str_list[i])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss History')\n",
    "    if (len(loss_history_list)) > 1:\n",
    "        plt.legend()\n",
    "    if savefig:\n",
    "        plt.savefig(f'loss_history_all_{savefig_str}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.01, 0.003, 0.0025, 0.002, 0.0015, 0.001, 0.0001, 0.00001]\n",
    "num_iterations = 100000\n",
    "\n",
    "loss_history_total = []\n",
    "accuracy_history_total = []\n",
    "error_history_total = []\n",
    "smallest_loss = None\n",
    "smallest_loss_lr = None\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    print(f\"=== Learning Rate is {learning_rate} ===\")\n",
    "\n",
    "    w, b = initialize_parameters(X.shape[0])\n",
    "\n",
    "    params, loss_history, accuracy_history, error_history = train(w, b, X, Y, num_iterations, learning_rate)\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "\n",
    "    final_loss, _ = propagate(w, b, X, Y)\n",
    "    final_accuracy = calculate_accuracy(w, b, X, Y)\n",
    "    final_error = calculate_error(w, b, X, Y)\n",
    "    \n",
    "    print(f\"Learning Rate: {learning_rate}, Final Train Accuracy: {final_accuracy:.16f}, Final Train Error: {final_error:.16f}, Final Train Loss: {final_loss:.16f}\")\n",
    "\n",
    "    plot_accuracy_history(accuracy_history, True, f'lr={learning_rate}')\n",
    "    plot_error_history(error_history, True, f'lr={learning_rate}')\n",
    "    plot_loss_history(loss_history, True, f'lr={learning_rate}')\n",
    "\n",
    "    accuracy_history_total.append(accuracy_history)\n",
    "    error_history_total.append(error_history)\n",
    "    loss_history_total.append(loss_history)\n",
    "\n",
    "    if smallest_loss is None:\n",
    "        smallest_loss = final_loss\n",
    "        smallest_loss_lr = learning_rate\n",
    "    else:\n",
    "        if final_loss < smallest_loss:\n",
    "            smallest_loss = final_loss\n",
    "            smallest_loss_lr = learning_rate\n",
    "\n",
    "plot_accuracy_history_all(accuracy_history_total, learning_rates)\n",
    "plot_error_history_all(error_history_total, learning_rates)\n",
    "plot_loss_history_all(loss_history_total, learning_rates)\n",
    "\n",
    "print(f\"The learning rate with smallest loss is {smallest_loss_lr}\")\n",
    "\n",
    "lr_index = learning_rates.index(smallest_loss_lr)\n",
    "check_accuracy_history = accuracy_history_total[lr_index]\n",
    "check_error_history = error_history_total[lr_index]\n",
    "check_loss_history = loss_history_total[lr_index]\n",
    "print(f'The largest accuracy appears first at epoch [{check_accuracy_history.index(max(check_accuracy_history)) + 1}] in the learning rate {smallest_loss_lr}.')\n",
    "print(f'The smallest error appears first at epoch [{check_error_history.index(min(check_error_history)) + 1}] in the learning rate {smallest_loss_lr}.')\n",
    "print(f'The smallest loss appears first at epoch [{check_loss_history.index(min(check_loss_history)) + 1}] in the learning rate {smallest_loss_lr}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
